{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba352899",
   "metadata": {},
   "source": [
    "# Unit 4 - Cloud Based Model Training \n",
    "In this notebook we will cover:\n",
    "   1. ***Very*** brief intro to Dask\n",
    "   2. Hosting the Numerai data in S3 \n",
    "   3. Training a model in the cloud with [Coiled](https://coiled.io/product/) and Dask !\n",
    "\n",
    "<img src=\"images/dawg.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02068ac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://kubedex.com/wp-content/uploads/2018/09/dask-logo.png\"\n",
    "     width=\"25%\"\n",
    "     alt=\"Dask logo\\\"/>\n",
    "     \n",
    "## Types of scaling problems in machine learning\n",
    "\n",
    "There are two main types of scaling challenges you can run into in your machine learning workflow: scaling the **size of your data** and scaling the **size of your model**. That is:\n",
    "\n",
    "1. **CPU-bound problems**: Data fits in RAM, but training takes too long. Many hyperparameter combinations, a large ensemble of many models, etc.\n",
    "2. **Memory-bound problems**: Data is larger than RAM, and sampling isn't an option.\n",
    "\n",
    " <img src=\"https://raw.githubusercontent.com/coiled/pydata-global-dask/master/images/grid_search_schedule.gif\"\n",
    "     width=\"50%\"\n",
    "     alt=\"Grid search schedule\\\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833fcdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When to use Dask?\n",
    "\n",
    "\n",
    "<img src=\"images/MemoryError.png\" \n",
    "     width=\"55%\"\n",
    "     alt=\"Dask overview\\\" />\n",
    "     \n",
    "     \n",
    "     \n",
    "Before trying to use Dask, there are some questions to determine if Dask might be suitable for you. \n",
    "- Does your data fit in memory? \n",
    "    - Yes: Use pandas or numpy.  \n",
    "    - No : Dask might be able to help. \n",
    "- Do your computations take for ever?\n",
    "    - Yes: Dask might be able to help. \n",
    "    - No : Awesome.\n",
    "- Do you have embarrassingly parallelizable code?\n",
    "    - Yes: Dask might be able to help.\n",
    "    - No?: If you are not sure here are some [examples](https://examples.dask.org/applications/embarrassingly-parallel.html) \n",
    "    - No: I'm sorry, although Dask might have some hope for you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117efaf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Bottom Left:** You don't need Dask.    \n",
    "**Elsewhere:** Dask fair game.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dask/dask-ml/main/docs/source/images/dimensions_of_scale.svg\"\n",
    "     width=\"50%\"\n",
    "     alt=\"Dask zones\">\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68572b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask-overview.svg\" \n",
    "     width=\"75%\"\n",
    "     alt=\"Dask overview\\\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a0737e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Multi-machine parallelism in the cloud with Coiled\n",
    "\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/coiled/data-science-at-scale/master/images/Coiled-Logo_Horizontal_RGB_Black.png\"\n",
    "     alt=\"Coiled logo\" \n",
    "     width=15%/>\n",
    "<br>\n",
    "\n",
    "Coiled, [among other things](https://coiled.io/product/), provides hosted and scalable Dask clusters.\n",
    "\n",
    "<img src=\"images/dask-gcp-bad-guy.png\"\n",
    "     alt=\"Coiled logo\" \n",
    "     width=55%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b282dde",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Alternatives\n",
    "\n",
    "There are **a lot** of alternatives to the setup used in this video, but ultimately I chose Dask/Coiled as it was the first I could get working\n",
    "\n",
    "Some of the alternative technologies/providers:\n",
    "\n",
    "<img src=\"images/options.png\"\n",
    "     width=\"75%\"\n",
    "     alt=\"Grid search schedule\\\" />\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dad954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8e23a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import coiled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc2d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spin up a Coiled cluster\n",
    "cluster = coiled.Cluster(software=\"peterling7710/pling_numerai\", \n",
    "                         backend_options={\"spot\": True}\n",
    "                         #worker_memory=\"16 GiB\",\n",
    "                        )\n",
    "\n",
    "cluster.adapt(minimum=2, maximum=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b1a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Client and print dashboard link\n",
    "client = Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if module versions are identical\n",
    "client.get_versions(check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb84f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access AWS environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "key  = 'ACCESS_KEY'\n",
    "ACCESS_KEY = os.getenv(key)\n",
    "\n",
    "key  = 'SECRET_KEY'\n",
    "SECRET_KEY = os.getenv(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b61bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "# Load S3 Bucket with AWS Credentials\n",
    "fs = s3fs.S3FileSystem(key=ACCESS_KEY, secret=SECRET_KEY)\n",
    "fs.ls('s3://numerai-data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba06fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(\n",
    "    \"s3://numerai-data/numerai_training_data_int8.csv\",\n",
    "    storage_options = {'key': ACCESS_KEY, 'secret': SECRET_KEY})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47873d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f3fd4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = [c for c in df if c.startswith(\"feature\")]\n",
    "features_erano = [c for c in df if c.startswith(\"feature\")] + [\"erano\"]\n",
    "\n",
    "targets = [c for c in df if c.startswith(\"target\")]\n",
    "\n",
    "df[\"erano\"] = df.era.astype(int)\n",
    "eras = df.erano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "filt = np.arange(1, 304, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample the data\n",
    "tdf = df.loc[df.erano.isin(filt)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce6d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the feature data from target data\n",
    "X, y = tdf[features_erano], tdf[\"target\"]\n",
    "\n",
    "y = y.to_frame()\n",
    "\n",
    "eras = X.erano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11091921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Dask Array and persist data to workers\n",
    "X_arr, y_arr = dask.persist(X.to_dask_array(lengths=True), y.to_dask_array(lengths=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn import model_selection, metrics \n",
    "from scipy.stats import spearmanr \n",
    "\n",
    "class TimeSeriesSplitGroups(_BaseKFold):\n",
    "    def __init__(self, n_splits=5):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_list = np.unique(groups)\n",
    "        n_groups = len(group_list)\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds ={0} greater\"\n",
    "                 \" than the number of samples: {1}.\").format(n_folds,\n",
    "                                                             n_groups))\n",
    "        indices = np.arange(n_samples)\n",
    "        test_size = (n_groups // n_folds)\n",
    "        test_starts = range(test_size + n_groups % n_folds,\n",
    "                            n_groups, test_size)\n",
    "        #test_starts = list(test_starts)[::-1]\n",
    "        for test_start in test_starts:\n",
    "            \n",
    "            yield (indices[groups.isin(group_list[:test_start])],\n",
    "                   indices[groups.isin(group_list[test_start:test_start + test_size])])\n",
    "\n",
    "def spearman(y_true, y_pred): \n",
    "    return spearmanr(y_pred, y_true).correlation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a35faa",
   "metadata": {},
   "source": [
    "# The Meta(verse?)\n",
    "\n",
    "- Gradient Boosting Decision Trees (GBDT) are a great starting point, and overall very well rounded algorithm\n",
    "- Several popular implementations of GBDT (Light GBM vs XGBoost vs. CatBoost)\n",
    "- We will be using LGBM for this series as it is very memory efficient\n",
    "    \n",
    "<img src=\"images/EZ_LGBM.jpg\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e8e968",
   "metadata": {},
   "source": [
    "<img src=\"images/SkleanJoblibDaskflow.png\"\n",
    "     width=\"50%\"\n",
    "     alt=\"Dask logo\\\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83edb34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# Create Joblib context with Dask backend to evaluate our models performance - quickly!\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    cvGen=TimeSeriesSplitGroups(n_splits=5) # purged cv\n",
    "    \n",
    "    for i,(train,test) in enumerate(cvGen.split(X=X_arr, y=y_arr, groups=eras)):\n",
    "        lgbm_model = lgb.DaskLGBMRegressor()\n",
    "        lgbm_model.fit(X_arr[train], y_arr[train])\n",
    "        \n",
    "        preds = lgbm_model.predict(X_arr[test])\n",
    "        score = spearman(y_arr[test], preds)\n",
    "        fold_scores.append(score)\n",
    "        \n",
    "        \n",
    "    print(fold_scores)\n",
    "\n",
    "    print(np.mean(fold_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d0430",
   "metadata": {},
   "source": [
    "# Thank You and Good Luck!\n",
    "- Like & Subscribe for more!\n",
    "- [Github](https://github.com/peterling7710/NumeraiStarterPack) with the notebooks for this series\n",
    "- Find my socials [here](https://linktr.ee/peterling) for more numer.ai related content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd49a6",
   "metadata": {},
   "source": [
    "<img src=\"images/TAF.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158767f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
